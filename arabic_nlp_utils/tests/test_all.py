"""
Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø´Ø§Ù…Ù„Ø© Ù„Ù…ÙƒØªØ¨Ø© arabic_nlp_utils
Comprehensive tests for arabic_nlp_utils library.
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import pytest
from arabic_nlp_utils import (
    # Normalizer
    normalize, normalize_alef, normalize_taa_marbuta,
    normalize_alef_maqsura, normalize_hamza, remove_tatweel,
    # Diacritics
    remove_diacritics, remove_harakat, remove_tanween, remove_shadda,
    has_diacritics, count_diacritics, diacritics_stats,
    extract_diacritized_words,
    # Cleaner
    clean_text, remove_urls, remove_emails, remove_mentions,
    remove_hashtags, remove_html_tags, remove_extra_spaces,
    remove_punctuation, remove_non_arabic, remove_emojis,
    reduce_repeated_chars,
    # Numbers
    to_western_numerals, to_arabic_numerals, to_eastern_numerals,
    extract_numbers, number_to_words, words_to_number,
    # Dialects
    detect_dialect, is_dialect, get_dialect_words, list_dialects,
    # Phonetics
    to_buckwalter, from_buckwalter, to_franco, to_phonetic, transliterate,
    # Tokenizer
    word_tokenize, simple_word_tokenize, sentence_tokenize,
    char_tokenize, remove_prefixes, remove_suffixes, segment,
    ngrams, char_ngrams,
    # Stopwords
    is_stopword, remove_stopwords, filter_stopwords, get_stopwords,
    stopword_count, stopword_ratio,
)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Normalizer Tests
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TestNormalizer:

    def test_normalize_alef(self):
        assert normalize_alef("Ø£Ø­Ù…Ø¯ Ø¥Ø¨Ø±Ø§Ù‡ÙŠÙ… Ø¢Ù…Ù†") == "Ø§Ø­Ù…Ø¯ Ø§Ø¨Ø±Ø§Ù‡ÙŠÙ… Ø§Ù…Ù†"

    def test_normalize_taa_marbuta(self):
        assert normalize_taa_marbuta("Ù…Ø¯Ø±Ø³Ø© Ø¬Ø§Ù…Ø¹Ø©") == "Ù…Ø¯Ø±Ø³Ù‡ Ø¬Ø§Ù…Ø¹Ù‡"

    def test_normalize_alef_maqsura(self):
        assert normalize_alef_maqsura("Ø¹Ù„Ù‰ Ù…ÙˆØ³Ù‰") == "Ø¹Ù„ÙŠ Ù…ÙˆØ³ÙŠ"

    def test_remove_tatweel(self):
        assert remove_tatweel("Ø§Ù„Ø¹Ù€Ù€Ù€Ù€Ø±Ø¨ÙŠØ©") == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
        assert remove_tatweel("Ø­Ù€Ù€Ù€Ø±ÙˆÙ") == "Ø­Ø±ÙˆÙ"

    def test_normalize_hamza(self):
        assert normalize_hamza("Ù…Ø³Ø¤ÙˆÙ„ Ø±Ø¦ÙŠØ³") == "Ù…Ø³Ø¡ÙˆÙ„ Ø±Ø¡ÙŠØ³"

    def test_normalize_pipeline(self):
        result = normalize("Ø£Ø­Ù…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù€Ù€Ø±Ø¨ÙŠØ©")
        assert "Ø§" in result  # alef normalized
        assert "Ù€" not in result  # tatweel removed

    def test_normalize_empty(self):
        assert normalize("") == ""

    def test_normalize_no_arabic(self):
        assert normalize("Hello World") == "Hello World"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Diacritics Tests
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TestDiacritics:

    def test_remove_diacritics(self):
        assert remove_diacritics("Ø¨ÙØ³Ù’Ù…Ù Ø§Ù„Ù„ÙÙ‘Ù‡Ù Ø§Ù„Ø±ÙÙ‘Ø­Ù’Ù…ÙÙ†Ù Ø§Ù„Ø±ÙÙ‘Ø­ÙÙŠÙ…Ù") == \
            "Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÙŠÙ…"

    def test_remove_harakat(self):
        assert remove_harakat("ÙƒÙØªÙØ¨Ù") == "ÙƒØªØ¨"

    def test_remove_tanween(self):
        assert remove_tanween("ÙƒØªØ§Ø¨Ù‹Ø§") == "ÙƒØªØ§Ø¨Ø§"

    def test_has_diacritics_true(self):
        assert has_diacritics("Ø¨ÙØ³Ù’Ù…Ù Ø§Ù„Ù„ÙÙ‘Ù‡Ù") is True

    def test_has_diacritics_false(self):
        assert has_diacritics("Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡") is False

    def test_count_diacritics(self):
        assert count_diacritics("Ø¨ÙØ³Ù’Ù…Ù") == 3

    def test_diacritics_stats(self):
        stats = diacritics_stats("Ø¨ÙØ³Ù’Ù…Ù Ø§Ù„Ù„ÙÙ‘Ù‡Ù")
        assert 'ÙƒØ³Ø±Ø©' in stats
        assert stats['ÙƒØ³Ø±Ø©'] >= 1

    def test_extract_diacritized_words(self):
        words = extract_diacritized_words("Ø¨ÙØ³Ù’Ù…Ù Ø§Ù„Ù„Ù‡ Ø§Ù„Ø±ÙÙ‘Ø­Ù’Ù…ÙÙ†Ù Ø§Ù„Ø±Ø­ÙŠÙ…")
        assert "Ø¨ÙØ³Ù’Ù…Ù" in words
        assert "Ø§Ù„Ù„Ù‡" not in words

    def test_remove_diacritics_empty(self):
        assert remove_diacritics("") == ""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Cleaner Tests
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TestCleaner:

    def test_remove_urls(self):
        text = "Ø²ÙˆØ±ÙˆØ§ https://example.com Ù„Ù„Ù…Ø²ÙŠØ¯"
        assert "https://example.com" not in remove_urls(text)

    def test_remove_emails(self):
        text = "ØªÙˆØ§ØµÙ„ Ù…Ø¹Ù†Ø§ test@example.com"
        assert "test@example.com" not in remove_emails(text)

    def test_remove_mentions(self):
        text = "Ù…Ø±Ø­Ø¨Ø§ @user ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ"
        assert "@user" not in remove_mentions(text)

    def test_remove_hashtags(self):
        text = "Ù…Ø±Ø­Ø¨Ø§ #Ø¹Ø±Ø¨ÙŠ"
        assert "#Ø¹Ø±Ø¨ÙŠ" not in remove_hashtags(text)

    def test_remove_html_tags(self):
        text = "<p>Ù…Ø±Ø­Ø¨Ø§ <b>Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…</b></p>"
        assert remove_html_tags(text) == "Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…"

    def test_remove_extra_spaces(self):
        assert remove_extra_spaces("Ù…Ø±Ø­Ø¨Ø§    Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…") == "Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…"

    def test_remove_punctuation(self):
        result = remove_punctuation("Ù…Ø±Ø­Ø¨Ø§! ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ")
        assert "!" not in result
        assert "ØŸ" not in result

    def test_remove_non_arabic(self):
        text = "Ù…Ø±Ø­Ø¨Ø§ Hello Ø¨Ø§Ù„Ø¹Ø§Ù„Ù… World"
        result = remove_non_arabic(text)
        assert "Hello" not in result
        assert "Ù…Ø±Ø­Ø¨Ø§" in result

    def test_reduce_repeated_chars(self):
        assert reduce_repeated_chars("Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡", 2) == "Ù‡Ù‡"
        assert reduce_repeated_chars("ÙŠÙŠÙŠÙŠÙŠ", 1) == "ÙŠ"

    def test_clean_text_pipeline(self):
        text = "Ù…Ø±Ø­Ø¨Ø§   Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…!! @user https://example.com ğŸ˜Š"
        result = clean_text(text)
        assert "@user" not in result
        assert "https://" not in result
        assert "ğŸ˜Š" not in result
        assert "  " not in result  # no double spaces

    def test_clean_text_empty(self):
        assert clean_text("") == ""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Numbers Tests
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TestNumbers:

    def test_to_western_numerals(self):
        assert to_western_numerals("Ù¡Ù¢Ù£") == "123"
        assert to_western_numerals("Ù¤Ù¥Ù¦Ù§Ù¨Ù©Ù ") == "4567890"

    def test_to_arabic_numerals(self):
        assert to_arabic_numerals("123") == "Ù¡Ù¢Ù£"

    def test_to_eastern_numerals(self):
        assert to_eastern_numerals("123") == "Û±Û²Û³"

    def test_extract_numbers(self):
        nums = extract_numbers("Ù„Ø¯ÙŠ Ù¢Ù£ ØªÙØ§Ø­Ø© Ùˆ 15 Ø¨Ø±ØªÙ‚Ø§Ù„Ø©")
        assert 23 in nums
        assert 15 in nums

    def test_number_to_words_zero(self):
        assert number_to_words(0) == "ØµÙØ±"

    def test_number_to_words_ones(self):
        assert number_to_words(1) == "ÙˆØ§Ø­Ø¯"
        assert number_to_words(5) == "Ø®Ù…Ø³Ø©"

    def test_number_to_words_teens(self):
        assert number_to_words(15) == "Ø®Ù…Ø³Ø© Ø¹Ø´Ø±"

    def test_number_to_words_tens(self):
        assert number_to_words(20) == "Ø¹Ø´Ø±ÙˆÙ†"

    def test_number_to_words_hundreds(self):
        result = number_to_words(123)
        assert "Ù…Ø¦Ø©" in result

    def test_number_to_words_thousands(self):
        result = number_to_words(1000)
        assert "Ø£Ù„Ù" in result

    def test_number_to_words_negative(self):
        result = number_to_words(-5)
        assert result.startswith("Ø³Ø§Ù„Ø¨")

    def test_words_to_number(self):
        assert words_to_number("Ø«Ù„Ø§Ø«Ø©") == 3
        assert words_to_number("ØµÙØ±") == 0
        assert words_to_number("Ø®Ù…Ø³Ø© Ø¹Ø´Ø±") == 15

    def test_mixed_digits(self):
        text = "Ø§Ù„Ø¹Ø¯Ø¯ Ù¡Ù¢Ù£ Ø£Ùˆ Û´ÛµÛ¶"
        result = to_western_numerals(text)
        assert "123" in result
        assert "456" in result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Dialects Tests
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TestDialects:

    def test_detect_egyptian(self):
        text = "Ø§Ù†Ø§ Ø¹Ø§ÙŠØ² Ø§Ø±ÙˆØ­ Ø§Ù„Ø¨ÙŠØª Ø¯Ù„ÙˆÙ‚ØªÙŠ Ø¹Ø´Ø§Ù† ØªØ¹Ø¨Ø§Ù† Ø§ÙˆÙŠ"
        results = detect_dialect(text)
        assert results[0]['dialect'] == 'egyptian'
        assert results[0]['score'] > 0

    def test_detect_gulf(self):
        text = "ÙˆØ´ ØªØ¨ÙŠ Ø§Ù„Ø­ÙŠÙ† ÙˆÙŠÙ† Ø±Ø§ÙŠØ­"
        results = detect_dialect(text)
        assert results[0]['dialect'] == 'gulf'

    def test_detect_levantine(self):
        text = "Ø´Ùˆ Ø¨Ø¯Ùƒ Ù‡Ù„Ù‚ ÙƒØªÙŠØ± Ù…Ù†ÙŠØ­"
        results = detect_dialect(text)
        assert results[0]['dialect'] == 'levantine'

    def test_detect_maghrebi(self):
        text = "ÙˆØ§Ø´ Ø¨ØºÙŠØª Ø¯ÙŠØ§Ù„ÙŠ Ø¨Ø²Ø§Ù Ù…Ø²ÙŠØ§Ù†"
        results = detect_dialect(text)
        assert results[0]['dialect'] == 'maghrebi'

    def test_is_dialect(self):
        assert is_dialect("Ø§Ù†Ø§ Ø¹Ø§ÙŠØ² Ø§Ø±ÙˆØ­", "egyptian") is True

    def test_list_dialects(self):
        dialects = list_dialects()
        assert len(dialects) >= 6
        names = [d['key'] for d in dialects]
        assert 'egyptian' in names
        assert 'gulf' in names

    def test_get_dialect_words(self):
        words = get_dialect_words("egyptian")
        assert len(words) > 0
        assert "Ø¹Ø§ÙŠØ²" in words

    def test_invalid_dialect(self):
        with pytest.raises(ValueError):
            get_dialect_words("unknown_dialect")

    def test_empty_text(self):
        results = detect_dialect("")
        assert len(results) > 0


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Phonetics Tests
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TestPhonetics:

    def test_to_buckwalter(self):
        assert to_buckwalter("Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡") == "bsm Allh"

    def test_from_buckwalter(self):
        assert from_buckwalter("bsm Allh") == "Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡"

    def test_buckwalter_roundtrip(self):
        original = "ÙƒØªØ§Ø¨"
        assert from_buckwalter(to_buckwalter(original)) == original

    def test_to_franco(self):
        result = to_franco("Ù…Ø±Ø­Ø¨Ø§")
        assert result == "mr7ba"

    def test_to_franco_with_7(self):
        result = to_franco("Ø­Ø¨")
        assert "7" in result

    def test_to_phonetic(self):
        result = to_phonetic("ÙƒØªØ¨")
        assert result == "ktb"

    def test_transliterate_buckwalter(self):
        assert transliterate("Ø¨Ø³Ù…", "buckwalter") == "bsm"

    def test_transliterate_franco(self):
        assert transliterate("Ù…Ø±Ø­Ø¨Ø§", "franco") == "mr7ba"

    def test_transliterate_invalid(self):
        with pytest.raises(ValueError):
            transliterate("Ù…Ø±Ø­Ø¨Ø§", "invalid_system")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Tokenizer Tests
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TestTokenizer:

    def test_word_tokenize(self):
        tokens = word_tokenize("Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠ!")
        assert tokens == ['Ù…Ø±Ø­Ø¨Ø§', 'Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…', 'Ø§Ù„Ø¹Ø±Ø¨ÙŠ']

    def test_simple_word_tokenize(self):
        tokens = simple_word_tokenize("Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…!")
        assert tokens == ['Ù…Ø±Ø­Ø¨Ø§', 'Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…!']

    def test_sentence_tokenize(self):
        text = "Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…. ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ Ø£Ù†Ø§ Ø¨Ø®ÙŠØ±!"
        sents = sentence_tokenize(text)
        assert len(sents) == 3

    def test_char_tokenize(self):
        assert char_tokenize("ÙƒØªØ§Ø¨") == ['Ùƒ', 'Øª', 'Ø§', 'Ø¨']

    def test_char_tokenize_with_spaces(self):
        result = char_tokenize("Ùƒ Øª", include_spaces=True)
        assert ' ' in result

    def test_remove_prefixes(self):
        assert remove_prefixes("ÙˆØ§Ù„ÙƒØªØ§Ø¨") == "ÙƒØªØ§Ø¨"
        assert remove_prefixes("Ø¨Ø§Ù„Ø¹Ù„Ù…") == "Ø¹Ù„Ù…"

    def test_remove_suffixes(self):
        assert remove_suffixes("ÙƒØªØ§Ø¨Ø§Øª") == "ÙƒØªØ§Ø¨"
        assert remove_suffixes("Ù…Ø¯Ø±Ø³ÙˆÙ†") == "Ù…Ø¯Ø±Ø³"

    def test_segment(self):
        result = segment("ÙˆØ§Ù„ÙƒØªØ§Ø¨Ø§Øª")
        assert result['prefix'] == "ÙˆØ§Ù„"
        assert result['stem'] == "ÙƒØªØ§Ø¨"
        assert result['suffix'] == "Ø§Øª"

    def test_ngrams(self):
        result = ngrams("Ø£Ù†Ø§ Ø£Ø­Ø¨ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©", 2)
        assert len(result) == 3
        assert result[0] == ('Ø£Ù†Ø§', 'Ø£Ø­Ø¨')

    def test_char_ngrams(self):
        result = char_ngrams("ÙƒØªØ§Ø¨", 2)
        assert result == ['ÙƒØª', 'ØªØ§', 'Ø§Ø¨']

    def test_word_tokenize_empty(self):
        assert word_tokenize("") == []


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Stopwords Tests
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TestStopwords:

    def test_is_stopword(self):
        assert is_stopword("ÙÙŠ") is True
        assert is_stopword("ÙƒØªØ§Ø¨") is False

    def test_remove_stopwords(self):
        text = "Ø£Ù†Ø§ Ø°Ù‡Ø¨Øª Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¯Ø±Ø³Ø© ÙÙŠ Ø§Ù„ØµØ¨Ø§Ø­"
        result = remove_stopwords(text)
        assert "Ø£Ù†Ø§" not in result
        assert "ÙÙŠ" not in result
        assert "Ø§Ù„Ù…Ø¯Ø±Ø³Ø©" in result

    def test_filter_stopwords(self):
        words = ["Ø£Ù†Ø§", "Ø£Ø­Ø¨", "Ø§Ù„Ù„ØºØ©", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"]
        filtered = filter_stopwords(words)
        assert "Ø£Ù†Ø§" not in filtered
        assert "Ø£Ø­Ø¨" in filtered

    def test_get_stopwords(self):
        sw = get_stopwords()
        assert len(sw) > 100
        assert "ÙÙŠ" in sw
        assert "Ù…Ù†" in sw

    def test_stopword_count(self):
        text = "Ø£Ù†Ø§ Ø°Ù‡Ø¨Øª Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¯Ø±Ø³Ø© ÙÙŠ Ø§Ù„ØµØ¨Ø§Ø­"
        count = stopword_count(text)
        assert count >= 3

    def test_stopword_ratio(self):
        text = "Ø£Ù†Ø§ Ø°Ù‡Ø¨Øª Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¯Ø±Ø³Ø©"
        ratio = stopword_ratio(text)
        assert 0.0 <= ratio <= 1.0

    def test_stopword_ratio_empty(self):
        assert stopword_ratio("") == 0.0


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Integration Tests
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TestIntegration:
    """Test that modules work together."""

    def test_clean_and_tokenize(self):
        text = "Ù…Ø±Ø­Ø¨Ø§!! @user https://x.com Ø¨Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠ ğŸ˜Š"
        cleaned = clean_text(text)
        tokens = word_tokenize(cleaned)
        assert len(tokens) >= 2

    def test_clean_and_stopwords(self):
        text = "Ø£Ù†Ø§ Ø£Ø­Ø¨ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙŠ ÙƒÙ„ Ù…ÙƒØ§Ù†"
        cleaned = clean_text(text, remove_diacritics_flag=False)
        result = remove_stopwords(cleaned)
        assert "Ø£Ù†Ø§" not in result
        assert "ÙÙŠ" not in result

    def test_normalize_and_diacritics(self):
        text = "Ø¨ÙØ³Ù’Ù…Ù Ø§Ù„Ù„ÙÙ‘Ù‡Ù Ø§Ù„Ø±ÙÙ‘Ø­Ù’Ù…ÙÙ†Ù Ø§Ù„Ø±ÙÙ‘Ø­ÙÙŠÙ…Ù"
        no_diac = remove_diacritics(text)
        normalized = normalize(no_diac)
        assert "Ù" not in normalized
        assert "Ù€" not in normalized

    def test_numbers_and_clean(self):
        text = "Ù„Ø¯ÙŠ Ù¢Ù£ ØªÙØ§Ø­Ø© ÙˆÙ¡Ù¥ Ø¨Ø±ØªÙ‚Ø§Ù„Ø©"
        western = to_western_numerals(text)
        nums = extract_numbers(western)
        assert 23 in nums
        assert 15 in nums

    def test_dialect_on_cleaned_text(self):
        text = "Ø§Ù†Ø§ Ø¹Ø§ÙŠØ² Ø§Ø±ÙˆØ­ Ø§Ù„Ø¨ÙŠØª @home Ø¯Ù„ÙˆÙ‚ØªÙŠ"
        cleaned = clean_text(text)
        results = detect_dialect(cleaned)
        assert results[0]['dialect'] == 'egyptian'


if __name__ == '__main__':
    pytest.main([__file__, '-v'])
